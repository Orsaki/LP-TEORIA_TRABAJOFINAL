import pandas as pd
import os
import time

# --- IMPORTACI√ìN DE TUS SCRAPERS ---
# Aseg√∫rate de que los nombres de los archivos (.py) y las funciones sean correctos
try:
    from web_scraping_el_comercio import extraer_noticias_comercio
    from webscraping_larepublica import extraer_noticias_larepublica
    from webscraping_canalN import extraer_noticias_canaln
    from webscraping_diariocorreo import extraer_noticias_correo
    # Si ya creaste el archivo de Per√∫21, descomenta la siguiente l√≠nea:
    from webscraping_peru21 import extraer_noticias_peru21
except ImportError as e:
    print(f"‚ùå Error de importaci√≥n: {e}")
    print("Verifica que los nombres de tus archivos .py sean exactos.")


def ejecutar_scrapers():
    """Ejecuta secuencialmente todos los scripts de scraping."""
    print("\nüöÄ INICIANDO PROCESO DE EXTRACCI√ìN DE NOTICIAS...")

    # 1. El Comercio
    try:
        extraer_noticias_comercio()
    except Exception as e:
        print(f"‚ö†Ô∏è Error en El Comercio: {e}")

    # 2. La Rep√∫blica
    try:
        extraer_noticias_larepublica()
    except Exception as e:
        print(f"‚ö†Ô∏è Error en La Rep√∫blica: {e}")

    # 3. Canal N
    try:
        extraer_noticias_canaln()
    except Exception as e:
        print(f"‚ö†Ô∏è Error en Canal N: {e}")

    # 4. Diario Correo
    try:
        extraer_noticias_correo()
    except Exception as e:
        print(f"‚ö†Ô∏è Error en Diario Correo: {e}")

    # 5. Per√∫ 21 (Opcional, si tienes el archivo)
    try:
        # Si no tienes el archivo a√∫n, comenta esta l√≠nea
        if 'extraer_noticias_peru21' in globals():
            extraer_noticias_peru21()
    except Exception as e:
        print(f"‚ö†Ô∏è Error en Per√∫21: {e}")


def unificar_csvs():
    """Busca los CSV generados y los une en uno solo."""
    print("\nüîÑ UNIFICANDO ARCHIVOS CSV...")

    # Lista de nombres exactos de los archivos que generan tus scripts
    archivos_generados = [
        "noticias_elcomercio_filtradas.csv",
        "noticias_larepublica_filtradas.csv",
        "noticias_canaln_filtradas.csv",
        "noticias_diariocorreo_filtradas.csv",
        "noticias_peru21_filtradas.csv"
    ]

    lista_dataframes = []

    for archivo in archivos_generados:
        if os.path.exists(archivo):
            try:
                # Leemos el CSV
                df = pd.read_csv(archivo)

                # Estandarizamos columnas (nos aseguramos de tener solo las necesarias)
                # Si tus CSV tienen columnas diferentes, esto evita errores al unir
                columnas_necesarias = ["Titulo", "Link", "Fuente"]

                # Verificamos que existan las columnas m√≠nimas
                if all(col in df.columns for col in columnas_necesarias):
                    df_filtrado = df[columnas_necesarias]
                    lista_dataframes.append(df_filtrado)
                    print(f"‚úÖ Integrado: {archivo} ({len(df)} registros)")
                else:
                    print(
                        f"‚ö†Ô∏è Formato incorrecto en {archivo}. Columnas encontradas: {df.columns}")

            except Exception as e:
                print(f"‚ùå Error leyendo {archivo}: {e}")
        else:
            print(f"‚ö™ No encontrado (se omitir√°): {archivo}")

    # Unir todo
    if lista_dataframes:
        df_total = pd.concat(lista_dataframes, ignore_index=True)

        # Eliminamos duplicados por LINK (mismo link = misma noticia)
        cant_inicial = len(df_total)
        df_total.drop_duplicates(subset=['Link'], keep='first', inplace=True)
        duplicados = cant_inicial - len(df_total)

        # Guardamos el resultado final
        nombre_final = "dataset_unificado.csv"
        df_total.to_csv(nombre_final, index=False, encoding='utf-8')

        print(f"\nüéâ PROCESO COMPLETADO.")
        print(f"üìä Total noticias recolectadas: {len(df_total)}")
        print(f"üóëÔ∏è  Duplicados eliminados: {duplicados}")
        print(f"üíæ Archivo guardado como: {nombre_final}")
    else:
        print("\n‚ùå No se encontraron datos para unificar.")


if __name__ == "__main__":
    # Paso 1: Ejecutar los robots
    ejecutar_scrapers()

    # Paso 2: Unificar la data
    unificar_csvs()
