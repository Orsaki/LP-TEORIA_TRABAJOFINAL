from config import HEADERS, DISTRITOS_INTEGRADOS, PALABRAS_CLAVE
import requests
from bs4 import BeautifulSoup
import re
import sys
import os
import time

# --- 1. CONEXI√ìN CON CONFIG.PY ---
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# --- 2. URLs OBJETIVO (CRIMEN PURO) ---
URLS_OBJETIVO = [
    "https://elcomercio.pe/noticias/delincuencia/",
    "https://elcomercio.pe/noticias/sicariato/",
    "https://elcomercio.pe/noticias/asaltos/",
    "https://elcomercio.pe/noticias/homicidios/",
    "https://elcomercio.pe/noticias/extorsion/",
    "https://elcomercio.pe/noticias/robo/"
]

# --- 3. LISTA NEGRA AMPLIADA (FILTRO "ANTI-GOBIERNO" Y "ANTI-CR√ìNICAS") ---
# Aqu√≠ agregamos las palabras espec√≠ficas de las filas que quieres eliminar.
BASURA_A_IGNORAR = [
    # A. Lo que pediste eliminar expl√≠citamente (Pol√≠tica/Gesti√≥n)
    "gobierno", "estado de emergencia", "pr√≥rroga", "decreto", "oficial",
    "guardia municipal", "serenos", "serenazgo", "funciones", "municipalidad",
    "alcalde", "norma", "ley", "congreso",

    # B. Cr√≥nicas o historias largas (El caso de la joyer√≠a)
    "cuentan c√≥mo", "vivir a un paso", "cr√≥nica", "historia de", "perfil",

    # C. Tr√°fico y Clima (Lo de siempre)
    "tr√°fico", "vehicular", "congestion", "desv√≠o",
    "navidad", "a√±o nuevo", "feriado", "celebraci√≥n", "misa",
    "senamhi", "clima", "verano", "playa", "calor",
    "incendio", "bomberos", "sismo", "temblor"
]


def buscar_palabra_exacta(texto, lista_palabras):
    texto = texto.lower()
    for palabra in lista_palabras:
        if re.search(r'\b' + re.escape(palabra) + r'\b', texto):
            return palabra.upper()
    return None


def obtener_noticias():
    noticias = []

    for url_base in URLS_OBJETIVO:
        # Solo p√°gina 1 para tener lo √∫ltimo y evitar basura antigua
        print(f"üì° Escaneando El Comercio: {url_base}...")

        try:
            response = requests.get(url_base, headers=HEADERS, timeout=10)
            if response.status_code != 200:
                continue

            soup = BeautifulSoup(response.content, 'html.parser')
            elementos = soup.find_all(['h2', 'h3'])

            for item in elementos:
                enlace = item.find('a')
                if enlace:
                    titulo = enlace.text.strip()
                    url_parcial = enlace.get('href')

                    if not url_parcial or len(titulo) < 15:
                        continue

                    url_noticia = "https://elcomercio.pe" + \
                        url_parcial if not url_parcial.startswith(
                            "http") else url_parcial

                    # ====================================================
                    # L√ìGICA DE LIMPIEZA
                    # ====================================================
                    titulo_lower = titulo.lower()

                    # 1. ELIMINAR FILAS NO DESEADAS
                    # Si contiene "Gobierno", "Emergencia", "Serenos" o "Cuentan c√≥mo", SE VA.
                    if any(basura in titulo_lower for basura in BASURA_A_IGNORAR):
                        continue

                    # 2. VALIDACI√ìN (Usando config.py)
                    distrito = buscar_palabra_exacta(
                        titulo, DISTRITOS_INTEGRADOS)
                    categoria = buscar_palabra_exacta(titulo, PALABRAS_CLAVE)

                    # 3. GUARDAR
                    if distrito and categoria:
                        if not any(n['Enlace'] == url_noticia for n in noticias):
                            noticias.append({
                                "Titular": titulo,
                                "Enlace": url_noticia,
                                "Fuente": "El Comercio",
                                "Distrito": distrito,
                                "Categor√≠a": categoria
                            })

        except Exception as e:
            print(f"Error leve en {url_base}: {e}")
            continue

        time.sleep(1)

    return noticias


if __name__ == "__main__":
    resultado = obtener_noticias()
    print(f"--- REPORTE FINAL ---")
    print(f"Noticias encontradas: {len(resultado)}")
    for n in resultado:
        print(
            f"‚úÖ {n['Titular']} \n   -> üìç {n['Distrito']} | üè∑Ô∏è {n['Categor√≠a']}\n")
